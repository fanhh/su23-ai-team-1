{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117\n",
      "0.14.0+cu117\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder_path = 'C:/Users/siddu/SU22_project/cursive_prediction/datafileszipped/datafiles'\n",
    "os.chdir(folder_path)\n",
    "files_in_folder = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dic = {}\n",
    "for file in files_in_folder:\n",
    "    letter = file[-5]\n",
    "    file_dic[file] = letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(file_dic.items()), columns=['file_name', 'letter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>letter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000_a.png</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001_b.png</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002_n.png</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_o.png</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_e.png</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>1935_i.png</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>1936_x.png</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>1937_o.png</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>1938_l.png</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>1939_r.png</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1940 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_name letter\n",
       "0     0000_a.png      a\n",
       "1     0001_b.png      b\n",
       "2     0002_n.png      n\n",
       "3     0003_o.png      o\n",
       "4     0004_e.png      e\n",
       "...          ...    ...\n",
       "1935  1935_i.png      i\n",
       "1936  1936_x.png      x\n",
       "1937  1937_o.png      o\n",
       "1938  1938_l.png      l\n",
       "1939  1939_r.png      r\n",
       "\n",
       "[1940 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def png_to_tensor(path):\n",
    "    # Replace 'path_to_image.png' with the actual path to your PNG file\n",
    "    image_path = folder_path + '/' + path\n",
    "\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    bw_image = image.convert('L')\n",
    "    data_transforms = [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(), # Scales data into [0,1] \n",
    "        transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1] \n",
    "    ]\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "    img_tensor = data_transform(bw_image)\n",
    "\n",
    "\n",
    "    return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO ADD NOISE \n",
    "def add_gaussian_noise(tensor, mean=0.0, std=0.1):\n",
    "    noise = torch.randn(tensor.size()) * std + mean\n",
    "    noisy_tensor = tensor + noise\n",
    "    return torch.clamp(noisy_tensor, min=-1.0, max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE TO NOISY TENSOR\n",
    "def png_to_noisy_tensor(path, noise_mean=0.0, noise_std=0.1):\n",
    "    # Replace 'path_to_image.png' with the actual path to your PNG file\n",
    "    image_path = folder_path + '/' + path\n",
    "\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    bw_image = image.convert('L')\n",
    "    data_transforms = [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(), # Scales data into [0,1] \n",
    "        transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1] \n",
    "    ]\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "    img_tensor = data_transform(bw_image)\n",
    "\n",
    "    # Add Gaussian noise to the tensor\n",
    "    noisy_img_tensor = add_gaussian_noise(img_tensor, mean=noise_mean, std=noise_std)\n",
    "\n",
    "    return noisy_img_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_image(tensor):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "    return reverse_transforms(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>letter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000_a.png</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001_b.png</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002_n.png</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_o.png</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_e.png</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>1935_i.png</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>1936_x.png</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>1937_o.png</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>1938_l.png</td>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>1939_r.png</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1940 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_name letter\n",
       "0     0000_a.png      a\n",
       "1     0001_b.png      b\n",
       "2     0002_n.png      n\n",
       "3     0003_o.png      o\n",
       "4     0004_e.png      e\n",
       "...          ...    ...\n",
       "1935  1935_i.png      i\n",
       "1936  1936_x.png      x\n",
       "1937  1937_o.png      o\n",
       "1938  1938_l.png      l\n",
       "1939  1939_r.png      r\n",
       "\n",
       "[1940 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image'] = df['file_name'].apply(png_to_tensor)\n",
    "df.drop(columns = ['file_name'], inplace = True)\n",
    "df = df[['image', 'letter']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5608, 0.5843, 0.6627,  ..., 0.6784, 0.6392, 0.5922],\n",
       "         [0.5373, 0.5843, 0.6549,  ..., 0.6706, 0.6392, 0.6000],\n",
       "         [0.4980, 0.5529, 0.6078,  ..., 0.6471, 0.6235, 0.6000],\n",
       "         ...,\n",
       "         [0.7333, 0.7255, 0.7020,  ..., 0.7333, 0.7647, 0.7725],\n",
       "         [0.7176, 0.7255, 0.7176,  ..., 0.7412, 0.7647, 0.7725],\n",
       "         [0.7176, 0.7255, 0.7255,  ..., 0.7412, 0.7647, 0.7725]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5608, 0.5843, 0.6627,  ..., 0.6784, 0.6392, 0.5922],\n",
       "         [0.5373, 0.5843, 0.6549,  ..., 0.6706, 0.6392, 0.6000],\n",
       "         [0.4980, 0.5529, 0.6078,  ..., 0.6471, 0.6235, 0.6000],\n",
       "         ...,\n",
       "         [0.7333, 0.7255, 0.7020,  ..., 0.7333, 0.7647, 0.7725],\n",
       "         [0.7176, 0.7255, 0.7176,  ..., 0.7412, 0.7647, 0.7725],\n",
       "         [0.7176, 0.7255, 0.7255,  ..., 0.7412, 0.7647, 0.7725]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    noise_df = df2.copy()\n",
    "    noise_df['image'] = noise_df['file_name'].apply(png_to_noisy_tensor)\n",
    "    noise_df.drop(columns = ['file_name'], inplace = True)\n",
    "    noise_df = noise_df[['image', 'letter']]    \n",
    "    df = df.append(noise_df, ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# df to tuples\n",
    "data_tuples = list(zip(df['image'].tolist(), df['letter'].tolist()))\n",
    "\n",
    "# Split indices\n",
    "dataset_size = len(data_tuples)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(0.2 * dataset_size)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Shuffle indices\n",
    "indices_shuffled = torch.randperm(len(indices))\n",
    "\n",
    "# Split \n",
    "train_indices, test_indices = indices_shuffled[split:], indices_shuffled[:split]\n",
    "\n",
    "# custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_tensor, letter = self.data[index]\n",
    "        return image_tensor, ord(letter) - ord('a')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# custom Datasets\n",
    "train_dataset = CustomDataset(data=[data_tuples[i] for i in train_indices])\n",
    "test_dataset = CustomDataset(data=[data_tuples[i] for i in test_indices])\n",
    "\n",
    "#  DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch: tensor([[[[0.9920, 0.8618, 0.8936,  ..., 0.9124, 1.0000, 0.8647],\n",
      "          [0.8649, 1.0000, 0.8967,  ..., 1.0000, 1.0000, 0.9845],\n",
      "          [1.0000, 0.9862, 1.0000,  ..., 0.8917, 0.9107, 0.7749],\n",
      "          ...,\n",
      "          [0.8898, 1.0000, 0.9452,  ..., 0.8701, 0.8830, 0.8917],\n",
      "          [1.0000, 0.9821, 1.0000,  ..., 0.7221, 0.9122, 0.8981],\n",
      "          [0.7282, 0.8308, 0.9136,  ..., 0.8818, 0.9985, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.9079, 0.8628, 0.7311,  ..., 1.0000, 1.0000, 0.9229],\n",
      "          [0.9372, 1.0000, 1.0000,  ..., 1.0000, 0.8420, 1.0000],\n",
      "          [0.9896, 0.8820, 1.0000,  ..., 0.9811, 0.8990, 0.9220],\n",
      "          ...,\n",
      "          [0.9321, 0.9100, 0.8139,  ..., 0.7332, 0.9023, 0.7061],\n",
      "          [1.0000, 0.8448, 0.9104,  ..., 0.7805, 0.8332, 0.9980],\n",
      "          [0.9145, 1.0000, 0.8948,  ..., 0.9236, 1.0000, 1.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.4941, 0.8042, 0.7008,  ..., 0.7046, 0.7213, 0.7000],\n",
      "          [0.4630, 0.4232, 0.4215,  ..., 0.6646, 0.6715, 0.7116],\n",
      "          [0.5198, 0.6364, 0.5741,  ..., 0.5221, 0.6978, 0.7676],\n",
      "          ...,\n",
      "          [0.5919, 0.6023, 0.5406,  ..., 0.5682, 0.6786, 0.6779],\n",
      "          [0.3817, 0.5626, 0.6718,  ..., 0.5280, 0.5966, 0.5999],\n",
      "          [0.5940, 0.5714, 0.5062,  ..., 0.6195, 0.6065, 0.7382]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.7023, 0.8619, 0.6329,  ..., 0.7102, 0.8974, 0.6435],\n",
      "          [0.8330, 0.6176, 0.8855,  ..., 0.7882, 0.7582, 0.6954],\n",
      "          [0.7308, 0.7116, 0.8349,  ..., 0.7228, 0.7748, 0.7384],\n",
      "          ...,\n",
      "          [0.6598, 0.7080, 0.5197,  ..., 0.6651, 0.6982, 0.7189],\n",
      "          [0.7284, 0.7889, 0.6352,  ..., 0.6413, 0.6208, 0.8603],\n",
      "          [0.6343, 0.6546, 0.8408,  ..., 0.8352, 0.6473, 0.6898]]],\n",
      "\n",
      "\n",
      "        [[[0.5840, 0.7341, 0.5066,  ..., 0.3416, 0.5934, 0.4936],\n",
      "          [0.7190, 0.5582, 0.5715,  ..., 0.8364, 0.6871, 0.6538],\n",
      "          [0.5735, 0.6901, 0.6689,  ..., 0.5988, 0.6739, 0.7221],\n",
      "          ...,\n",
      "          [0.7659, 0.6514, 0.6306,  ..., 0.7848, 0.6125, 0.6893],\n",
      "          [0.6325, 0.7054, 0.7236,  ..., 0.7992, 0.6364, 0.9414],\n",
      "          [0.6186, 0.4472, 0.6923,  ..., 0.5969, 0.7560, 0.6601]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.8518, 0.8379,  ..., 0.7290, 0.8638, 0.8139],\n",
      "          [1.0000, 1.0000, 0.9074,  ..., 0.6564, 0.8326, 0.9995],\n",
      "          [0.9424, 0.8520, 0.9635,  ..., 0.9970, 0.9180, 0.7392],\n",
      "          ...,\n",
      "          [0.3390, 0.4232, 0.1589,  ..., 0.4135, 0.4518, 0.3264],\n",
      "          [0.3104, 0.4419, 0.5692,  ..., 0.3998, 0.4334, 0.2638],\n",
      "          [0.4122, 0.5869, 0.7502,  ..., 0.5054, 0.1524, 0.1506]]]])\n",
      "y batch: tensor([14, 18, 17,  7, 19,  3, 14, 20,  4, 20,  4, 20, 24, 14,  0,  5,  5,  2,\n",
      "        12, 12, 11,  7,  0, 13, 12, 11, 14,  4, 10,  0, 18,  0])\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in test_loader:\n",
    "    print(\"X batch:\", X_batch)\n",
    "    print(\"y batch:\", y_batch)\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AlexNet architecture\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=26):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_15508\\2453823309.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.763547610477562\n",
      "Epoch 2, Loss: 1.5188484206963122\n",
      "Epoch 3, Loss: 0.5341495398697111\n",
      "Epoch 4, Loss: 0.28531629010452453\n",
      "Epoch 5, Loss: 0.15577447173915532\n",
      "Epoch 6, Loss: 0.10665389693347381\n",
      "Epoch 7, Loss: 0.10711046971034299\n",
      "Epoch 8, Loss: 0.04698897054397966\n",
      "Epoch 9, Loss: 0.06163578690383941\n",
      "Epoch 10, Loss: 0.019740690448519473\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "alexnet = AlexNet().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(alexnet.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = alexnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "print(\"Training Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_15508\\3622266950.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.018601914496087633\n",
      "Testing Finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss = 0.0\n",
    "\n",
    "alexnet.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "        outputs = alexnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n",
    "print(\"Testing Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    x = inputs\n",
    "    y = labels\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.8357e-01,  9.1988e-01,  9.3131e-01,  ...,  1.0000e+00,\n",
       "           1.0000e+00,  1.0000e+00],\n",
       "         [ 8.6466e-01,  8.1068e-01,  8.5065e-01,  ...,  8.6495e-01,\n",
       "           8.9474e-01,  9.6549e-01],\n",
       "         [ 6.9174e-01,  6.7081e-01,  1.0000e+00,  ...,  9.2094e-01,\n",
       "           9.9560e-01,  7.6849e-01],\n",
       "         ...,\n",
       "         [ 7.1836e-01,  6.1316e-01,  8.1793e-01,  ...,  1.4899e-01,\n",
       "          -7.8275e-02,  1.1268e-01],\n",
       "         [ 9.3123e-01,  8.8014e-01,  6.8307e-01,  ..., -7.8497e-04,\n",
       "          -3.1421e-02,  3.3042e-02],\n",
       "         [ 8.8881e-01,  6.9963e-01,  9.1540e-01,  ...,  5.0580e-01,\n",
       "           6.6258e-01,  7.3611e-01]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_input_reshaped = np.expand_dims(x, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(image_tensor):\n",
    "    # Ensure the input tensor is on the same device as the model\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # Perform the forward pass\n",
    "    with torch.no_grad():\n",
    "        output = alexnet(image_tensor)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x36 and 9216x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15508\\295379973.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15508\\3744941607.py\u001b[0m in \u001b[0;36mpred\u001b[1;34m(image_tensor)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Perform the forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malexnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\siddu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15508\\660521683.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\siddu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\siddu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\siddu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\siddu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x36 and 9216x4096)"
     ]
    }
   ],
   "source": [
    "pred(x)[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_tensor_image(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
